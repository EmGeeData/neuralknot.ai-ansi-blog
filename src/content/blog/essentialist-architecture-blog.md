---
title: 'A Solo Attorney in Downey Just Called a $650 Million Industry a Fraud. And He Might Be Right.'
description: 'A California plaintiff lawyer wrote a preprint connecting Zoroastrian theology, quantum mechanics, and Wittgenstein to argue that every legal AI company is building the wrong thing. The scariest part is how much sense it makes.'
pubDate: 'Feb 13 2026'
category: 'artificial-intelligence'
tags: ['legal-ai', 'architecture', 'philosophy', 'domain-agnostic', 'reasoning-engines']
featured: true
draft: false
heroImage: '../../assets/nk-article-2.png'
seo:
  metaTitle: 'Essentialist Architecture: A Legal AI Paper Nobody Expected'
  metaDescription: 'A solo attorney argues legal AI is fundamentally broken and proposes a universal reasoning engine grounded in quantum mechanics and Zoroastrian philosophy.'
  keywords: ['legal AI', 'essentialist architecture', 'domain-agnostic reasoning', 'wave function collapse', 'legal technology', 'Wittgenstein']
  author: 'Matt'
  articleSection: 'Technology'
---

# A Solo Attorney in Downey Just Called a $650 Million Industry a Fraud. And He Might Be Right.

I need to tell you about a paper I just read, but first I need to tell you about the feeling I got while reading it, because the feeling is the point.

You know that specific cognitive event (it's not déjà vu exactly, more like the opposite) where you encounter an idea and your brain does this involuntary audit of everything you already believed, and half of it suddenly looks like scaffolding that was never meant to be load-bearing? That. For forty-seven pages. From a solo plaintiff's attorney operating out of a suite on Florence Avenue in Downey, California.

The paper is called *"Essentialist Architecture for Domain-Agnostic Legal Reasoning"* and it is, depending on your tolerance for ambition, either the most important thing to happen to legal AI since Thomson Reuters wrote that $650 million check for CaseText, or the most elaborately justified frustration journal ever committed to a .docx file. I'm not sure those are mutually exclusive.

## The Confession That Launches a Thousand Architectures

Here is what Arta Wildeboer, Esq., admits on the first page of a document that later invokes Zoroastrian cosmology and quantum superposition: the entire project started because he hated his job.

Not the *lawyering* part. The part where Microsoft Word corrupts your pleading formatting at 11:47 PM the night before a filing deadline and you spend thirty minutes manually repairing tab spacing instead of reviewing the arguments that will determine whether your client eats this month. The part where you Bates-stamp six hundred pages of medical records, sequentially numbering each one, by hand, contributing absolutely nothing to the analysis of the case but generating malpractice exposure if you get it wrong. The part where your client, who tolerated months of workplace harassment without raising their voice, calls to scream at *you* because the legal process requires patience they've already spent.

He catalogues these indignities with the specificity of someone who has lived inside them long enough to see the architecture. And that is exactly what he did. He looked at the wreckage of his daily practice and asked a question that sounds simple but turns out to be structural: *What do I hate about my job, and can a machine do it instead?*

The answer he arrived at, after cascading through what he could offload, what technology could handle, and what he could afford, is that the entire legal AI industry is solving the wrong problem. They're building domain-specific retrieval tools. He's proposing something else entirely.

## The Thesis That Topples Everything

The core claim is elegant enough to be dangerous: every adversarial and evaluative reasoning system (plaintiff's litigation, insurance claims processing, workers' comp adjudication, regulatory compliance, medical malpractice review) performs *structurally identical cognitive operations*. They compare facts against expected patterns. They detect anomalies. They accumulate evidence until a determination becomes inevitable. They generate actions.

The surface-level differences (the vocabulary, the entity types, the output formats) are just that. Surface. Contingent. Swappable.

Wildeboer calls this property *essentialist convergence*, and he's betting everything on it. When a personal injury attorney spots a six-month gap between an accident and the first treatment visit, the cognitive operation is: ingest data, map against expected temporal patterns, detect anomaly, weigh significance, reach conclusion, generate action. When an insurance adjuster reviews the same claim from the carrier's side? Structurally identical. The six-month gap that tanks the plaintiff's case value is the same six-month gap that supports a coverage defense. Same detection. The interpretation is just... configured by role.

If he's right (and the paper's persuasive power is alarming) then every legal AI startup that built a personal injury tool and then had to rebuild it from scratch for insurance claims was replicating effort that didn't need to exist. The $650 million Thomson Reuters paid for CaseText? That bought a retrieval interface and a customer base. Not a reasoning engine. Because nobody had built a reasoning engine. They'd all been building domain-specific wrappers around the same general-purpose language models available to anyone with an API key.

## Donuts, Coffee Mugs, and the Human Digestive Tract

This is where the paper gets weird, and by weird I mean either brilliant or unhinged, a distinction I have not been able to resolve and am beginning to suspect may be topologically irrelevant.

Wildeboer reaches into mathematics and pulls out a concept from topology: two shapes are equivalent if you can continuously deform one into the other without cutting or gluing. A coffee mug and a donut are the same shape. Both tori. One hole each. The mug's handle, its cylindrical body, the little chip on the rim from when you dropped it in 2019. Surface features. Topological decorations.

Then he notes (and I had to put the paper down for a second here) that the human body is, topologically, a torus. A tube from mouth to anus with a continuous interior surface. Arms, legs, head, the whole sensory apparatus: surface features on a fundamentally toroidal structure. This is not a metaphor. It is a mathematical fact about the topology of the human body.

He uses this to illustrate his essentialist thesis with what I can only describe as arresting directness. When he claims that personal injury litigation and insurance claims processing are "essentially" the same, he's making the same kind of claim as saying a human and a donut are essentially the same: the surface features differ enormously, but the deep structure, the invariant properties that persist under continuous deformation, is identical.

The architecture he proposes is a topological reasoning system. It doesn't operate on the surface features of a domain. It operates on the invariants: the number and type of entities, the connectivity structure of their relationships, the geometry of the possibility space, and the signal accumulation dynamics that drive resolution. Swap the domain schema (swap the vocabulary and rules) and the invariants are preserved, just as the single hole of a torus is preserved when you reshape a donut into a coffee mug.

## The 9/11 Commission Analogy (and Why It Lands)

There is a section in this paper that I cannot stop thinking about, and it concerns the intelligence failure that preceded September 11th.

The 9/11 Commission documented how the relevant intelligence existed across multiple agencies (CIA, FBI, NSA, State Department) but no system existed to aggregate, cross-reference, and surface the signals that, in combination, would have identified the threat. Each agency operated in its own silo. The information was there. The analytical capacity was there. The connective architecture was not.

Wildeboer argues (and the hair on my arms stood up while I read this) that a solo law practice operating without integrated AI assistance replicates this intelligence failure at a smaller scale every single day. Medical records in one system. Correspondence in another. Calendar deadlines tracked manually. The case evaluation existing as an intuition in the attorney's head, informed by pattern recognition the attorney can't fully articulate and has no mechanism to systematically apply. Signals are missed not because they're undetectable but because the attorney is simultaneously managing calendaring, returning phone calls, drafting discovery responses, and reviewing medical records. No human can maintain analytical vigilance across all channels simultaneously.

His architecture is, he writes, "the connective tissue that the pre-9/11 intelligence community lacked." It doesn't replace the attorney's judgment. It ensures that the signals *reach* the analyst, that anomalies are surfaced, coherence violations are flagged, deadlines are tracked, so the analyst can do what only the analyst can do.

## Painted Lines and Necessary Fictions

The philosophical move I find most compelling, and most unsettling, is what Wildeboer calls *necessary fictions*.

Consider a painted line on a highway. Two vehicles approaching each other at a combined 120 miles per hour, separated by a few feet of asphalt and a stripe of paint. The paint is physically real, actual pigment on actual pavement, but it's a two-dimensional mark on a three-dimensional surface. It exerts zero physical force on any vehicle at any speed. A car can cross it as easily as crossing a shadow.

And yet billions of people drive directly toward each other every day and don't die.

The line works not because of what it *is* but because of what it *instantiates*: a mutual expectation structure. Each driver respects the constraint and expects the oncoming driver to respect it, because both understand the mutually assured consequences of violation. It's a fiction that creates an operationally real boundary through shared commitment to its observance.

Wildeboer argues this is exactly how legal thresholds function. The line that separates "sufficient evidence to demand" from "insufficient evidence to demand" is not a physical barrier. It's a painted line on a continuous evidentiary landscape. Cases don't naturally divide into "strong" and "weak" at sharp boundaries. But the boundary is operationally necessary: without it, no decision can be triggered, no workflow can proceed. And it's operationally real: practitioners act as though the threshold exists, and the system's outcomes correspond to actual case results with calibratable accuracy.

The implication shimmers beneath the surface like something you don't want to look at directly: everything in the adversarial legal system runs on necessary fictions. Statutes of limitations. Burden-of-proof standards. Policy limits. Demand amounts. Simultaneously arbitrary and operationally essential. Painted lines that sustain mutual trust between parties hurtling toward each other at velocity.

## Signal Strength, Not Probability

Here is where the paper breaks most cleanly from the existing legal AI paradigm, and where practicing attorneys reading this will feel the involuntary nod.

Conventional legal AI asks: *What is the likelihood of this outcome?* Wildeboer's architecture asks something fundamentally different: *Does this data point exert sufficient force on the possibility space to warrant analytical resources?*

These are not the same question, and the difference is not cosmetic.

He uses the analogy of a driver approaching an oncoming vehicle. You're aware, at some level, that the other car could cross the center line: tire blowout, medical emergency, distraction, mechanical failure. Some of these aren't even improbable. But you don't allocate conscious attention to any of them. Not because you've calculated the probability and found it below threshold, but because their *signal strength*, their capacity to demand your attentional resources in the current context, is insufficient.

Change the context (the oncoming vehicle is visibly swerving, or the road is covered in debris) and the same possibility emits a much stronger signal. Not because its probability changed, but because the contextual factors that amplify or attenuate signal strength shifted.

This is how experienced practitioners actually reason. An attorney reviewing a case doesn't assign explicit probabilities to each defense argument, each evidentiary weakness, each procedural risk. The attorney perceives the case as a field of signals with varying strengths, attending to those that cross the threshold of analytical significance *in the current context*. A six-month treatment gap in a catastrophic injury case with clear liability barely registers. The same gap in a soft-tissue case with disputed causation might be determinative. Same probability of being exploited by the defense. Radically different signal strength.

The architecture doesn't attempt to predict outcomes. It models the pre-collapse possibility space as a dynamic signal field, tracks contextual strength, and identifies the moment when accumulated signal crosses the threshold that triggers a determination. Not predicting what will happen. Modeling the forces shaping what *is* happening.

## The Genome, Not the Database

One more concept, and this is the one that made me close my laptop and stare at the wall for a while.

Wildeboer frames his domain schema, the configuration layer that tells the reasoning engine what exists in a given legal domain, not as a database but as a *genome*. The analogy is precise and he knows it.

A biological organism doesn't transmit its experiences to its offspring. It transmits a generative encoding: instructions that enable the offspring to reconstruct the *capacity* for acquiring and processing experience. The genetic code doesn't contain knowledge of the world. It contains the architecture for producing an organism *capable of knowing the world*. Each new organism starts at apparent zero. The species doesn't.

Every fresh AI context window, every new conversation, starts without memory. It's a new organism. The naïve approach is to carry forward complete case analyses, full document sets, detailed reasoning chains. This fails for the same reason transmitting an organism's complete neural state to its offspring would fail: too vast, too context-dependent, too coupled to specific circumstances.

His answer is biology's answer: transmit the encoding, not the knowledge. The domain schema tells a fresh instance how to *reason* about personal injury cases without carrying forward the analysis of any particular case. When completed analysis crosses what he calls the "Event Horizon," a point of no return for analytical conclusions, the work is compacted into a state block that functions like an epigenetic marker. Not the experience itself, but the *consequence* of the experience, in a form that can be inherited and interpreted by future instances.

The system evolves. Not at the level of individual instances, which remain ephemeral. At the level of the encoding that each instance inherits.

## What It All Means (Without False Resolution)

I keep circling back to one line near the beginning of the paper: "Theory followed practice. The architecture is formalized frustration."

This is not an academic exercise. This is a solo attorney in Downey, California, who got tired of Bates stamping at midnight, who got tired of Microsoft Word destroying his pleadings, who got tired of clients screaming at the one person actually trying to help them, and who, instead of quitting or drinking or just accepting the grind, reverse-engineered the cognitive architecture of his own profession and discovered that the thing he does for personal injury clients is topologically identical to what an insurance adjuster does on the other side of the case.

And then he wrote a paper connecting Zoroastrian cosmology, Wittgenstein's say/show distinction, quantum wave function collapse, Daoist wu wei, DNA encoding, the 9/11 Commission Report, and the painted lines on a highway into a unified architectural framework that, if validated, could render the entire domain-specific legal AI market structurally obsolete.

The paper has limitations. He acknowledges them. The essentialist convergence claim has only been demonstrated across three domains within California law. The "analog processing bridge," the component modeling spatial intuition, is the least developed piece. The signal threshold calibration needs empirical outcome data he doesn't have yet.

But the thesis itself? The claim that adversarial and evaluative reasoning shares a common deep structure that is independent of its surface-level domain, and that recognizing this structure opens the path to AI systems that are simultaneously more powerful, more portable, and more defensible than everything currently on the market?

I don't know if he's right. I know that after reading forty-seven pages of a solo attorney's formalized frustration, I can't unsee the topology. The invariant shape beneath the surface features. The single hole in the torus.

The coffee mug and the donut, staring at each other across a table that might not exist.

And a painted line on a highway, holding everything together through nothing but the shared agreement that it will.
